Congratulations! You passed!
Grade received 80%
To pass 80% or higher
1.
Question 1
When performing logistic regression on sentiment analysis, you represented each tweet as a vector of ones and zeros. However your model did not work well. Your training cost was reasonable, but your testing cost was just not acceptable. What could be a possible reason?

1 / 1 point

The vector representations are sparse and therefore it is much harder for your model to learn anything that could generalize well to the test set.


You probably need to increase your vocabulary size because it seems like you have very little features.


Logistic regression does not work for sentiment analysis, and therefore you should be looking at other models.


Sparse representations require a good amount of training time so you should train your model for longer

Correct
This is correct.

2.
Question 2
Which of the following are examples of text preprocessing?

1 / 1 point

Stemming, or the process of reducing a word to its word stem.

Correct
This is correct.


Lowercasing, which is the process of removing changing all capital letter to lower case.

Correct
This is correct.


Removing stopwords, punctuation, handles and URLs

Correct
This is correct.


Adding new words to make sure all the sentences make sense

3.
Question 3
The sigmoid function is defined as h( x^{(i)}, \theta) =
\frac{1}{1+e^{-\theta^Tx^{(i)}}}h(x 
(i)
 ,θ)= 
1+e 
−θ 
T
 x 
(i)
 
 
1
​
 . Which of the following is true.

1 / 1 point

Large positive values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h( x^{(i)}, \theta)h(x 
(i)
 ,θ) closer to 1 and large negative values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h(
x^{(i)},\theta)h(x 
(i)
 ,θ) close to -1.


Large positive values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h( x^{(i)}, \theta)h(x 
(i)
 ,θ) closer to 1 and large negative values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h(
x^{(i)},\theta)h(x 
(i)
 ,θ) close to 0.


Small positive values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h( x^{(i)}, \theta)h(x 
(i)
 ,θ) closer to 1 and large positive values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h(
x^{(i)},\theta)h(x 
(i)
 ,θ) close to 0.


Small positive values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h( x^{(i)}, \theta)h(x 
(i)
 ,θ) closer to 0 and large negative values of \theta^Tx^{(i)}θ 
T
 x 
(i)
  will make h(
x^{(i)},\theta)h(x 
(i)
 ,θ) close to -1.

Correct
This is correct.

4.
Question 4
The cost function for logistic regression is defined as J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log h\left(x^{(i)},
\theta\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)},
\theta\right)\right)\right]J(θ)=− 
m
1
​
 ∑ 
i=1
m
​
 [y 
(i)
 logh(x 
(i)
 ,θ)+(1−y 
(i)
 )log(1−h(x 
(i)
 ,θ))]. Which of the following is true about the cost function above. Mark all the correct ones.

1 / 1 point

When y^{(i)} = 1y 
(i)
 =1, as h(x^{(i)},\theta)h(x 
(i)
 ,θ) goes close to 0, the cost function approaches \infty∞.

Correct
This is correct.


When y^{(i)} = 1y 
(i)
 =1, as h(x^{(i)},\theta)h(x 
(i)
 ,θ) goes close to 0, the cost function approaches 00.


When y^{(i)} = 0y 
(i)
 =0, as h(x^{(i)},\theta)h(x 
(i)
 ,θ) goes close to 0, the cost function approaches 00.

Correct
This is correct.


When y^{(i)} = 0y 
(i)
 =0, as h(x^{(i)},\theta)h(x 
(i)
 ,θ) goes close to 0, the cost function approaches \infty∞.

5.
Question 5
For what value of \theta^Txθ 
T
 x in the sigmoid function does h(x^{(i)},\theta) = 0.5h(x 
(i)
 ,θ)=0.5.

0 / 1 point
0.5
Incorrect
Anything other than 0, will not work. Feel free to plug it into the equation.

6.
Question 6
Select all that apply. When performing logistic regression for sentiment analysis using the method taught in this week's lecture, you have to:

1 / 1 point

Performing data processing.

Correct
This is correct.


Create a dictionary that maps the word and the class that word is found in to the number of times that word is found in the class.

Correct
This is correct.


Create a dictionary that maps the word and the class that word is found in to see if that word shows up in the class.


For each tweet, you have to create a  positive feature with the sum of positive counts of each word in that tweet. You also have to create a negative feature with the sum of negative counts of each word in that tweet.

Correct
This is correct.

7.
Question 7
When training logistic regression, you have to perform the following operations in the desired order.

1 / 1 point

Initialize parameters, get gradient, classify/predict, update, get loss, repeat


Initialize parameters, classify/predict, get gradient, update, get loss, repeat


Initialize parameters, get gradient, update, classify/predict, get loss, repeat


Initialize parameters, get gradient, update, get loss, classify/predict, repeat

Correct
This is correct.

8.
Question 8
Assuming we got the classification correct, where y^{(i)} = 1y 
(i)
 =1 for some specific example i. This means that h(x^{(i)}, \theta) > 0.5h(x 
(i)
 ,θ)>0.5. Which of the following has to hold:

0 / 1 point

Our prediction, h(x^{(i)}, \theta)h(x 
(i)
 ,θ) for this specific training example is exactly equal to its corresponding label y^{(i)}y 
(i)
 .


Our prediction, h(x^{(i)}, \theta)h(x 
(i)
 ,θ) for this specific training example is less than (1 - y^{(i)}1−y 
(i)
 ).


Our prediction, h(x^{(i)}, \theta)h(x 
(i)
 ,θ) for this specific training example is less than (1 - h(x^{(i)}, \theta))(1−h(x 
(i)
 ,θ)).


Our prediction, h(x^{(i)}, \theta)h(x 
(i)
 ,θ) for this specific training example is greater than (1 - h(x^{(i)}, \theta))(1−h(x 
(i)
 ,θ)).

Incorrect
This is not correct. Prediction could not be negative.

9.
Question 9
What is the purpose of gradient descent? Select all that apply.

1 / 1 point

Gradient descent allows us to learn the parameters \thetaθ in logistic regression as to minimize the loss function J.

Correct
This is correct.


Gradient descent allows us to learn the parameters \thetaθ in logistic regression as to maximize the loss function J.


Gradient descent,  grad_theta allows us to update the parameters \thetaθ by computing \theta = \theta - \alpha * grad\_thetaθ=θ−α∗grad_theta

Correct
This is correct.


Gradient descent,  grad_theta allows us to update the parameters \thetaθ by computing \theta = \theta + \alpha * grad\_thetaθ=θ+α∗grad_theta

10.
Question 10
What is a good metric that allows you to decide when to stop training/trying to get a good model? Select all that apply.

1 / 1 point

When your accuracy is good enough on the test set.

Correct
This is correct.


When your accuracy is good enough on the train set.


When you plot the cost versus (# of iterations) and you see that your the loss is converging (i.e. no longer changes as much).

Correct
This is correct.


When \alphaα, your step size is neither too small nor too large.
